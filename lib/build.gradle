plugins {
    id 'scala'
    id 'java-library'
    id 'maven-publish'
    id 'signing'
}

final def scalaVersion = getScalaVersion()
final def scalaLibraryVersion = getScalaLibraryVersion()

final def sparkVersion = getSparkVersion()
final def sparkLibraryVersion = getSparkLibraryVersion()

dependencies {
    // We use Spark in API, so clients will need it when compiling
    // In most cases, the actual Spark code should be provided by the
    // execution envrionment, so compileOnlyApi is more technically correct than api
    // although the generated POM is the same regardless
    compileOnlyApi "org.apache.spark:spark-core_$scalaVersion:$sparkLibraryVersion"
    compileOnlyApi "org.apache.spark:spark-sql_$scalaVersion:$sparkLibraryVersion"
    compileOnlyApi "org.apache.spark:spark-catalyst_$scalaVersion:$sparkLibraryVersion"

    compileOnly "org.scala-lang:scala-library:$scalaLibraryVersion"

    implementation "com.squareup.okhttp3:okhttp:4.10.0"
    // 2.13.5 is the last release that supports Java 8, and Java 8 is the only official
    // supported for Databricks. Java 11 is only in preview for Databricks 10.
    implementation "com.github.plokhotnyuk.jsoniter-scala:jsoniter-scala-core_$scalaVersion:2.13.5"
    implementation "com.github.plokhotnyuk.jsoniter-scala:jsoniter-scala-macros_$scalaVersion:2.13.5"
    implementation "io.reactivex.rxjava3:rxjava:3.1.5"

    testImplementation "org.apache.spark:spark-core_$scalaVersion:$sparkLibraryVersion"
    testImplementation "org.apache.spark:spark-sql_$scalaVersion:$sparkLibraryVersion"
    testImplementation "org.scalatest:scalatest_$scalaVersion:3.0.8"
    testImplementation "junit:junit:4.13.2"
}

repositories {
    mavenCentral()
}

java {
    toolchain {
        targetCompatibility = JavaLanguageVersion.of(8)
    }
}

scaladoc {
    title = 'Joom Spark Platform'
}

task sourcesJar(type: Jar) {
    archiveClassifier = 'sources'
    from sourceSets.main.allScala
}

task javadocJar(type: Jar) {
    archiveClassifier = 'javadoc'
    def scaladoc = tasks.scaladoc
    from scaladoc.destinationDir
    dependsOn scaladoc
}

publishing {
    publications {
        maven(MavenPublication) {
            groupId = 'com.joom.spark'
            artifactId = "spark-platform_$scalaVersion"
            version = '0.4.7'

            from components.java

            artifact tasks.sourcesJar
            artifact tasks.javadocJar

            pom {
                name = 'spark-platform'
                description = 'Foundational Spark tool from Joom.'
                inceptionYear = '2022'
                url = 'https://github.com/joomcode/spark-platform'

                licenses {
                    license {
                        name = 'MIT License'
                        url = 'http://www.opensource.org/licenses/mit-license.php'
                        distribution = 'repo'
                    }
                }
                organization {
                    name = 'Joom'
                    url = 'https://joom-group.com'
                }
                developers {
                    developer {
                        id = 'vprus'
                        name = 'Vladimir Prus'
                        email = 'vladimir.prus@gmail.com'
                    }
                }
                scm {
                    connection = 'scm:git:git://github.com/joomcode/spark-platform.git'
                    developerConnection = 'scm:git:ssh://git@github.com/joomcode/spark-platform.git'
                    url = 'https://github.com/joomcode/spark-platform'
                }
            }
        }
    }

    repositories {
        def mavenUrl = findProperty('publication.repository.maven.url')
        if (mavenUrl != null) {
            maven {
                url mavenUrl

                def mavenName = findProperty('publication.repository.maven.name')
                def mavenUsername = findProperty('publication.repository.maven.username')
                def mavenPassword = findProperty('publication.repository.maven.password')

                if (mavenName != null) {
                    name mavenName
                }
                credentials {
                    username mavenUsername
                    password mavenPassword
                }
            }
        }
    }
}

signing {
    def isSigningEnabled = findProperty('publication.signing.enabled')?.toBoolean() ?: true
    if (isSigningEnabled) {
        def keyId = findProperty('publication.signing.keyId')
        def secretKey = findProperty('publication.signing.secretKey')
        def password = findProperty('publication.signing.password')
        if (secretKey != null && password != null) {
            useInMemoryPgpKeys(keyId, secretKey, password)
            sign publishing.publications.maven
        }
    }
}

getProperty('java.test.versions')
    .split(',')
    .collect { JavaLanguageVersion.of(it).asInt() }
    .unique()
    .each { majorVersion ->
        def jdkTest = tasks.register("testJdk$majorVersion", Test) {
            javaLauncher = javaToolchains.launcherFor {
                languageVersion = JavaLanguageVersion.of(majorVersion)
            }

            description = "Runs the test suite on JDK $majorVersion"
            group = LifecycleBasePlugin.VERIFICATION_GROUP

            // Copy inputs from normal Test task.
            def testTask = tasks.getByName("test")
            classpath = testTask.classpath
            testClassesDirs = testTask.testClassesDirs
        }
        tasks.named("check").configure { dependsOn(jdkTest) }
    }

String getScalaLibraryVersion() {
    def scalaVersion = getProperty('scala.version').toString()
    def parts = scalaVersion.split('\\.')
    if (parts.length >= 3) {
        return scalaVersion
    } else {
        return (parts + '+').join('.')
    }
}

String getScalaVersion() {
    def scalaVersion = getProperty('scala.version').toString()
    def parts = scalaVersion.split('\\.')
    switch (parts[0]) {
        case '2':
            return "${parts[0]}.${parts[1]}"
        default:
            throw new IllegalArgumentException("Unsupported scala version $scalaVersion")
    }
}

String getSparkLibraryVersion() {
    def scalaVersion = getProperty('spark.version').toString()
    def parts = scalaVersion.split('\\.')
    if (parts.length >= 3) {
        return scalaVersion
    } else {
        return (parts + '+').join('.')
    }
}

String getSparkVersion() {
    def scalaVersion = getProperty('spark.version').toString()
    def parts = scalaVersion.split('\\.')
    switch (parts[0]) {
        case '3':
            return "${parts[0]}.${parts[1]}"
        default:
            throw new IllegalArgumentException("Unsupported Spark version $scalaVersion")
    }
}
